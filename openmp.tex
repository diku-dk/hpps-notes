\lstset{language=C}

\chapter{OpenMP}
\label{chap:openmp}

Writing multi-threaded programs using the raw POSIX threads API is
tedious and error-prone.  It is also not portable, as POSIX threads is
specific to Unix systems.  Also, writing efficient multi-threaded code
is difficult, as thread creation is relatively expensive, so we should
ideally write our programs to have a fixed number of \emph{worker
  threads} that are kept running in the background, and periodically
assigned work by a scheduler.  In many cases, particularly within
scientific computing, we do not need the flexibility and low-level
control of POSIX threads.  We mostly wish to parallelise loops with
iterations that are \emph{independent}, meaning that they can be
executed in any order without changing the result.  For such programs
we can use \emph{OpenMP}.  We will use only a small subset of OpenMP
in this course.

\section{Basic use of OpenMP}

OpenMP is an extension to the C programming language\footnote{OpenMP
  is not C specific, and is also in wide use for Fortran.} that allows
the programmer to insert high-level \emph{directives} that indicate
when and how loops should be executed in parallel.  The compiler and
runtime system then takes care of low-level thread management.  OpenMP
uses the the \emph{fork-join} model of parallel execution: a program
starts with a single thread, the \emph{master thread}, which runs
sequentially until the first \emph{parallel region} (such as a
parallel loop) is encountered.  At this point, the master thread
creates a group of threads (``fork''\footnote{Note an unfortunate
  mix-up of nomenclature: this has nothing to do with the Unix notion
  of \texttt{fork()}, which creates \emph{processes}, not
  \emph{threads}}), which execute the loop.  The master thread waits
for all of them to finish (``join''), and then continues on
sequentially.  This is called an \textit{implicit barrier}: a point
where execution pauses until all threads reach it.  For example:

\begin{lstlisting}[mathescape=true]
#pragma omp parallel for
for (int i=0; i<n; i++) {
  A[i] = A[i]*2;
}
\end{lstlisting}

The \lstinline{#pragma omp parallel for} line is an OpenMP directive
that indicates that the iterations of the following \texttt{for} loop
can be executed in parallel.  If this loop is compiled with a compiler
that supports OpenMP, the \texttt{n} iterations of the loop will be
divided among some worker threads, which will then execute them in
parallel.

The number of threads used can be controlled at run-time, and is
usually not equal to the number of iterations in the parallel loop.
This is because when the amount of work per iteration is small (as
above), it would not be efficient to have one thread per iteration.

One important idea behind OpenMP is that to understand the semantics
of a program, we can always remove the directives and consider what
the remaining sequential C program would compute.  This is called the
\emph{sequential elision}.  This is a great advantage over low-level
multi-threaded programming.

When we ask OpenMP to parallelise a loop, we solemnly swear that the
following \lstinline{for} loop is actually parallel.  If we break this
vow then the parallel and sequential execution of the code will give
different results; the API does not provide any guarantees about the
absence of race-conditions.  For example, the iterations of the
following loop are not independent, yet OpenMP will not stop us from
asking it to be executed in parallel:

\begin{lstlisting}[mathescape=true]
sum = 0;
#pragma omp parallel for
for (int i=0; i<N; i++) {
  sum += A[i];
}
\end{lstlisting}

Since all threads executing the parallel region have access to the
same data, it is easy to have accidental race conditions in OpenMP
programs.  In \cref{chap:dependencies} we will look in detail at
determining when it is safe to execute a loop in parallel, and how to
transform loops so they become safe to execute in parallel.

\subsection{Compiling and running OpenMP programs}

To compile with support for OpenMP directives in \texttt{gcc}, pass
the \texttt{-fopenmp} option to the compiler. The number of threads
that are going to be used for parallel execution can be set by
environment variable \texttt{OMP\_NUM\_THREADS}.  For example,

\begin{verbatim}
$ export OMP_NUM_THREADS=8
\end{verbatim}

sets the environment variable for the current shell session, such that
any OpenMP we run will use eight threads.  Determining the optimal
number of threads to use for a given program on a particular machine
is something of a black art.  In practice, we just try a few different
numbers and see what runs fastest.

\begin{figure}
\lstinputlisting[
caption={A very simple example of using OpenMP.},
label={lst:openmp-example},
language=C,
frame=single]
{src/openmp-example.c}
\end{figure}

For example, suppose the contrived program in
\cref{lst:openmp-example} is stored in the file
\texttt{openmp-example.c}.  We can then compile as follows:

\begin{verbatim}
$ gcc -o openmp-example openmp-example.c -fopenmp
\end{verbatim}

And then run with various values of \lstinline{OMP_NUM_THREADS} to
investigate the impact of parallelisation:

\begin{verbatim}
$ time OMP_NUM_THREADS=1  ./openmp-example
real    0m0.124s
user    0m0.034s
sys     0m0.090s
$ time OMP_NUM_THREADS=2  ./openmp-example
real    0m0.076s
user    0m0.033s
sys     0m0.104s
$ time OMP_NUM_THREADS=4  ./openmp-example
real    0m0.054s
user    0m0.039s
sys     0m0.133s
$ time OMP_NUM_THREADS=8  ./openmp-example
real    0m0.046s
user    0m0.054s
sys     0m0.184s
\end{verbatim}

Note how the \emph{real} time drops as we use more threads---although
it's not quite eight times as fast with eight threads as with one.
This is likely because this contrived program does so little work
compared to the amount of memory we are accessing.

\subsection{Parallelism versus Concurrency}

The terms \emph{parallelism} and \emph{concurrency} are frequently and
historically used interchangeably.  If you look then up in a
dictionary, you will find them to have almost the same definitions.
In computer science, they are terms of art with distinct (although
related) meanings.

To illustrate concurrency, consider a video game, which from a
programming perspective is basically a real-time interactive
simulation.  Many things need to happen concurrently:

\begin{itemize}
\item We need to figure out what sounds and music to play and send it
  to the IO device connected to the speakers.
\item Many times per second, we need to draw to the screen a rendering
  of the world as observed by the player.
\item Perhaps we wish to guess at where the player is headed next, and
  preload those parts of the game world.
\item We need to run artificial intelligence for computer-controlled
  enemies.
\item We need to compute physics interactions.
\item In a multiplayer game, we need to transmit information
  information about the local game world to other players across the
  network, as well as incorporate information we receive in return.
\item Probably we also wish to perform cleanup tasks, such as removing
  objects from the game world that after a while are no longer
  necessary (e.g. the remains of deceased enemies), to clear up system
  resources.
\end{itemize}

From a programming perspective, it is nicer if we can write each of
these parts as separate flows of control. The artificial intelligence
code should not worry about constantly checking whether it's time to
draw a new screen frame, or whether the player hit some key.
Similarly, the rate at which we receive information from the network
is completely unpredictable, relative to our other responsibilities.

One solution is to implement each of these parts as a distinct thread,
and depend on the operating system to \emph{context-switch} between
them as needed. Maybe we can also use some form of \emph{scheduling
  policy} that understands that it's more important for the threads
responsible for music and graphics to run when they need to, than the
thread that cleans up dead objects or simulates physics. If we have
only a single processor, then all these different threads run
\emph{concurrently}, in that they overlap in time, but only one will
physically be executing instructions at any given point in time. This
means that concurrency can be a useful programming model even when the
goal is not to make the program faster. For that matter, threads are
not the only way to implement concurrency---asynchronous event loops
are a popular technique for highly scalable web servers, but outside
the scope of this course.

\begin{definition}[Concurrency]
  Concurrency is the use of multiple, possibly interacting, logical
  flows of control.
\end{definition}

\emph{Parallelism} is about making programs faster by performing
several computations at the same time.  When we have a program with
multiple threads, such as the video game example above, then if we
have a machine with more than one processing core (which is
essentially every machine these days), we can run several of those
threads in parallel.  While in Unix, threads are the fundamental
\emph{implementation mechanism} for obtaining parallelism, we often
program in languages or frameworks that do not directly expose
threads, because they can be difficult to work with.  For example,
OpenMP is a parallel programming model, but for simple parallel loops,
we do not concern ourselves with actual threads, and we only have a
single logical control flow.

\begin{definition}[Parallelism]
  Parallelism is the simultaneous use of multiple processing units,
  with the goal of speeding up a computation.
\end{definition}

In scientific computing we are mostly concerned with parallelising
loops with independent iterations, and not with concurrent flows of
control.  While OpenMP allows us to peek beneath the covers and
interact with the threads that it uses to \textit{implement} the
parallel loop abstraction, there does exist forms of parallelism, and
parallel programming languages, that do not expose this abstraction,
and are truly parallel without exposing any concurrency.

\section{Reductions}
\label{sec:reductions}

A loop whose iterations are completely independent can be parallelised
with the \lstinline{#pragma omp parallel for} directive, as shown
before.  Another common case is when the iterations are
\emph{almost} independent, but they all update a single accumulator
- for example, when summing the elements of an array.  For such loops,
OpenMP provides \emph{reduction clauses}, as used to compute a dot
product on \cref{lst:openmp-dotprod}.

\begin{figure}
\lstinputlisting[
caption={OpenMP dot product with reduction clause.},
label={lst:openmp-dotprod},
language=C,
frame=single,
lastline=8]
{src/openmp-dotprod.c}
\end{figure}

Note that all iterations of the loop update the same \texttt{sum}
variable. The reduction clause \texttt{reduction(+:sum)} that we added
to the OpenMP directive indicates that this update is done with the
\lstinline{+} operator. The compiler will transform this loop such
that each thread gets its own \emph{private} copy of \texttt{sum},
which they then update independently. At the end, these per-thread
\emph{partial results} are then combined to obtain the final result.

Reduction clauses only immediately work with a small set of built-in
binary operators: \lstinline{+}, \lstinline{*},
\lstinline{&&}, \lstinline{||}, \lstinline{&}, \lstinline{|},
\lstinline{^}, \lstinline{max}, and \lstinline{min}.  It is also
possible to use user-defined functions, but this is beyond the scope
of this text.  The common property shared by these operators is that
they are \emph{associative}, and have a \emph{neutral element}.  By
associativity, we mean that for some operator $\oplus$, we have
\[
  (x \oplus y) \oplus z = x \oplus (y \oplus z).
\]
That is, the ``order of evaluation'' does not matter. This is what
allows us to partition the iterations of a reduction loop between
multiple threads, without changing the result. Note that subtraction
and division is \emph{not} associative---this is we don't use them in
reduction clauses.

A neutral element $0_{\oplus}$ for some operator $\oplus$ is a
``natural zero'' that does not change the result of evaluation:
\[
  x \oplus 0_{\oplus} = 0_{\oplus} \oplus x = x
\]
For example, if the operator is addition, then the neutral element is
$0$.  If the operator is multiplication, then the neutral element is
$1$.  OpenMP requires that the initial value of each reduction
variable (\lstinline{sum} for \cref{lst:openmp-dotprod}) is the
neutral element of the operator.

\subsection{Associativity of floating point operations}

Some of you might recall that addition and multiplication of
floating-point numbers is \emph{not} associative, due to roundoff
errors.  Yet we perform a reduction on \lstinline{double} values in
\cref{lst:openmp-dotprod}!  How can that be valid?  The short answer
is that OpenMP allows us to shoot ourselves in the foot if we wish.
In practice, floating-point operations are often \emph{almost}
associative, and we can get useful results by treating them as if they
were.  In particular, there is no reason to believe that a sequential
left-to-right summation of floating-point numbers is going to be more
numerically accurate than a parallelisation of that loop.  This does
mean that an OpenMP program that uses reductions on floating-point
values might compute a different result than the original sequential
program.

\section{Nested loops}
\label{sec:openmp-nesting}

We can prefix every parallelisable \lstinline{for}-loop with an OpenMP
parallelisation directive.  But what happens if we nest multiple
parallel loops such as the following?

\begin{lstlisting}
#pragma omp parallel for
for (int i = 0; i < n; i++) {
#pragma omp parallel for
  for (int j = 0; j < m; j++) {
    ...
  }
}
\end{lstlisting}

In principle, the answer is easy: the original master thread launches
worker threads to handle each of the \texttt{n} outer iterations, and
these individual worker threads may then launch more worker threads to
handle the inner \texttt{m} iterations.  This is called \emph{nested
  parallelism}: iterations of a parallel loop may itself contain more
parallel loops.  The overhead of nested parallelism quickly becomes
significant, in particular if we have recursive functions so that the
nesting is \emph{dynamic}, so in practice it is not widely used in
OpenMP.  In fact, OpenMP implementations are likely to ignore nested
parallelisation directives, unless the \lstinline{OMP_NESTED}
environment variable is set to \lstinline{True} when running the
program.

However, a common case of nested parallelism is iterating across all
elements of a multidimensional array, such as above, where we are
conceptually covering all indexes of an \texttt{n} by \texttt{m}
array.  This is called \emph{regular nested parallelism}, because the
iteration count of the inner loop (\texttt{m}) is \emph{invariant}
(the same) for all iterations of the outer loop.  Such a nested loop
can be collapsed to a single loop that performs \texttt{n*m} iterations:

\begin{minipage}{1.0\linewidth}
\begin{lstlisting}
#pragma omp parallel for
for (int ij = 0; ij < n*m; ij++) {
  int i = ij / m;
  int j = ij % m;
  ...
}
\end{lstlisting}
\end{minipage}

We use division and modulo operations to extract the intended indexes
from the ``combined'' index \texttt{ij}---this is actually the inverse
of the row-major two-dimensional index function.

However, writing code like this is not very nice, as it obscures our
intent.  Fortunately, OpenMP provides the \texttt{collapse} clause
that we can use to tell OpenMP to parallelise multiple \emph{perfectly
  nested} loops.  By perfectly nested, we mean that the outermost
loops contain only a loop, and no other statements.  An example is
shown in \cref{lst:openmp-matadd}, where we tell OpenMP to treat the
two-deep \emph{loop nest} as a single parallel loop.

\begin{figure}[!h]
\lstinputlisting[
caption={Matrix addition with OpenMP.},
label={lst:openmp-matadd},
language=C,
frame=single,
lastline=10]
{src/openmp-matadd.c}
\end{figure}

You should generally use the \texttt{collapse} clause when writing
such perfectly nested loops, which occurs frequently when implementing
matrix operations.  But more generally, it is usually not worth
worrying too much about parallelising all inner loops of an OpenMP
program.  All we have to do is provide enough parallel work such that
all processors on the system have work to do, and as of this writing,
even a very large computer is unlikely to have more than 256 CPU
cores---and on a personal computer, 16 is more likely.

\section{Scheduling}

When we use a directive to ask OpenMP to execute a loop in parallel,
the compiler and runtime system will decide how the iterations should
be distributed among the threads.  By default, OpenMP uses
\emph{static scheduling}.

\begin{definition}[Static scheduling]
  When entering a parallel loop, we assign each thread exactly the
  number of iterations to execute.
\end{definition}

For example, when executing a loop with $n$ iterations on $m$ threads,
we might assign $\frac{n}{m}$ iterations to each thread.

Static scheduling can be non-optimal when a loop is not
\emph{load-balanced}, meaning that not all loop iterations take the
same time.  For such an \emph{imbalanced} loop, some threads may
finish their iterations quickly and then sit idle while the other
threads finish theirs.  In such cases, we can ask OpenMP to use
\emph{dynamic scheduling}.

\begin{definition}[Dynamic scheduling]
  When entering a parallel loop, we assign each thread an iteration.
  When a thread finishes an iteration, it receives a new one to
  execute.
\end{definition}

The advantage of dynamic scheduling is that an idle thread will
receive more work (if any is available).  The disadvantage is that
dynamic scheduling requires additional communication and
synchronisation---while this is done for us by the runtime system, it
carries a performance overhead.

As an example of the benefit of dynamic scheduling, consider
parallelising loops where each iteration computes Fibonacci numbers
using the recursive function defined in \cref{lst:cfib}\footnote{This
  is an inefficient way to compute Fibonacci numbers---we only use as
  an expensive computation that will take some time.}.

\begin{figure}
\lstinputlisting[
caption={Recursive Fibonacci function.},
label={lst:cfib},
language=C,
frame=single,
firstline=4,
lastline=10]
{src/openmp-schedule.c}
\end{figure}

Since computation of $fib(i+1)$ takes over twice the time of $fib(i)$,
we can produce a very imbalanced loop by letting iteration $i$ compute
$fib(i)$.  \Cref{lst:openmp-fib-static} shows how to parallelise this
with a static schedule in OpenMP.  On my machine, for \texttt{n=45},
this program runs in $5.2s$.  We can ask for dynamic scheduling by
using the the \texttt{schedule(dynamic)} clause, as shown on
\cref{lst:openmp-fib-dynamic}.  On my machine, this version runs in
$2.27s$ - a speedup (\cref{sec:speedup}) of $2.29\times$.

\begin{figure}
\lstinputlisting[
caption={Fibonacci loop with static scheduling.},
label={lst:openmp-fib-static},
language=C,
frame=single,
firstline=19,
lastline=22]
{src/openmp-schedule.c}
\end{figure}

\begin{figure}
\lstinputlisting[
caption={Fibonacci loop with dynamic scheduling.},
label={lst:openmp-fib-dynamic},
language=C,
frame=single,
firstline=27,
lastline=30]
{src/openmp-schedule.c}
\end{figure}

By default, dynamic scheduling assigns single loop iterations to
threads.  This is not a problem for the Fibonacci example, because
there are few loop iterations, and they take a fairly long time to
run.  For loops with many iterations, where dynamically scheduling
single iterations at a time would involve too much overhead, we can
add a \emph{chunk size} to the scheduling clause.  For example,
\begin{lstlisting}
#pragma omp parallel for schedule(dynamic, 100)
\end{lstlisting}
will schedule in chunks of 100 iterations at a time.

The \texttt{guided} schedule is similar to \texttt{dynamic}, but
starts out with big chunks and may then decrease the chunk size during
program run-time if the work is imbalanced.

OpenMP's default behaviour will usually do a good job scheduling most
loops.  But if we do not see the performance gains we would expect,
and we see in a process monitor that some of our processors are idle
while our program is running, it is worth considering whether our
loops could benefit from a scheduling clause.

\section{Parallel Partitioning}
\label{sec:parallel-partitioning}

Most uses of OpenMP involve loops where the iterations are fully
independent and can be executed in parallel. \Cref{sec:reductions}
showed how certain patterns of not-entirely-parallel loops can be
handled via reduction clauses. However, reduction clauses are a rather
inflexible mechanism. In this section we will look at a more general
approach to parallelisation. The overall idea is to break a problem
apart in independent subproblems, solve each subproblem independently,
them combine them at the end.

\subsection{List homomorphisms}
\label{sec:list-homomorphisms}

\newcommand\concat{\mathbin{+\mkern-10mu+}}

The theoretical basis for our approach is \emph{list homomorphisms}.
For the purpose of this section, a list is a mathematical object
containing an ordered sequence of objects, not specifically a linked
list. Lists are written as zero or more objects closed in square
brackets, the concatenation of two lists $x$ and $y$ is written as
$x\concat y$.

\begin{example}[List concatenation]
  \begin{align}
    [1,2,3]&\concat[4,5,6] &&= [1,2,3,4,5,6] \\
    []&\concat[1,2,3] &&= [1,2,3] \\
    [1,2,3]&\concat[] &&= [1,2,3]
  \end{align}
\end{example}

There are many operations we may wish to perform on lists: summing
them, sorting them, filtering them, and so on. In many cases, we can
perform such operations by splitting the list into two or more parts,
process the parts independently, then combine the results of
processing the parts. This is similar to the principle of \emph{divide
  and conquer} that you may be familiar with from algorithmics. A list
homomorphism is exactly a function that be computed in this manner.

\begin{definition}[List homomorphism]
  A function $h$ on lists is a list homomorphism when there exists
  some associative binary operator $\odot{}$ such that

\[
  h (x \concat y) = h x \odot h y
\]
for any $x$ and $y$.
\end{definition}

\begin{example}[Summation is a list homomorphism with $\odot=+$]
  \[
    \textrm{sum}(x \concat y) = sum(x) + sum(y)
  \]
\end{example}

Sometimes we leave the case of an empty input implicit, in which case
it is the neutral element for the operator. In the case of summation,
that is zero. We may also allow the case of a single-element argument
(or any finite length) to be specified explicitly.

\begin{example}[Filtering negative numbers is a list homomorphism]
  \[
    \begin{array}{l}
      \textrm{filter}([x]) =
      \begin{cases}
        [x] & x \geq 0 \\
        [] & x < 0 \
      \end{cases}\\
      \textrm{filter}(x \concat y) = \textrm{filter}(x) \concat \textrm{filter}(y)
    \end{array}
  \]
\end{example}

This does not strictly speaking extend the expressivity of list
homomorphisms, as this behaviour could otherwise be encoded in
$\odot$.

One useful property when implementing list homomorphisms on a computer
is that the ``recursive'' invocations of the homomorphism need not be
implemented in the same way, by splitting the input into tiny parts.
When computing a list homomorphism we can split the input into any
number of chunks, sequentially compute a result per chunk (perhaps
using a different algorithm), and then combine the results into a
final result for the whole list. Each chunk can be processed
independently of the others, in parallel. We will now turn our
attention to how to implement this idea with OpenMP.

For futher information on list homomorphisms, see Bird's \emph{An
  Introduction to the Theory of Lists}~\cite{theoryoflists} and
Gibbons' \emph{The Third Homomorphism
  Theorem}~\cite{gibbons1995third}.

\subsection{Explicit Work Partioning in OpenMP}

When using OpenMP, our ``lists'' are arrays (or iterations of a loop),
and we do not usually physically split and concatenate them, as moving
data around in memory is expensive. We will also recursively split
problems into ever smaller parts, as one might do in a classic
divide-and-conquer problem. In general, the problem of how to
partition work among processors can be quite challenging, but our
approach will be rather simple: if we have $P$ processors available,
we will split the problem into $P$ independent parts. To do this in
OpenMP, we need to have more fine grained control of how the parallel
execution is performed. Specifically, we must make use of OpenMP's
notion of a \emph{parallel region}, as distinct from a parallel loop.

A parallel region is written as the directive \texttt{\#pragma omp
  parallel} followed by a brace-delimited block (\emph{not} a loop).
When a parallel region is encountered, it is executed by each of the
threads. For example, the code
\begin{lstlisting}[language=C]
#pragma omp parallel
{
  printf("hello!\n");
}
\end{lstlisting}
would result in a line of output from each thread (perhaps
intermixed). Each thread executing a parallel region is executing the
same code. However, a thread can use the function
\texttt{omp\_get\_thread\_num()} (available from \texttt{omp.h}) to
retrieve its number as an integer, which can be used to make different
threads perform different tasks---or operate on different parts of the
input. This approach to parallel computing is known as \emph{data
  parallelism}.

\begin{definition}[Data parallelism]
  Simultaneously performing \textit{the same} operation on different
  pieces of the same data.
\end{definition}

\begin{figure}
\lstinputlisting[
caption={Vector addition with loop iterations automatically distributed among the thread.},
label={lst:openmp-vecadd-implicit},
language=C,
frame=single,
firstline=1,
lastline=8]
{src/openmp-vecadd.c}
\end{figure}

\begin{figure}
\lstinputlisting[
caption={Vector addition with explicit distribution of work. Note the absence of \textbf{for} in the OpenMP directive.},
label={lst:openmp-vecadd-explicit},
language=C,
frame=single,
firstline=10,
lastline=30]
{src/openmp-vecadd.c}
\end{figure}


\Cref{lst:openmp-vecadd-implicit} shows an implementation of vector
addition using an OpenMP directive to split the iterations of a loop
among the threads, with the specifics decided by the runtime system.
This is the approach we saw previously.

\Cref{lst:openmp-vecadd-explicit} shows another way of parallelising
vector addition, where we explicitly carve the input into chunks of
size $\frac{n}{P}$ and let each of the $P$ threads process one chunk.
Given vectors of size $n$ and $P$ threads, we can make thread $t$
compute the iterations in the range
$[t\cdot \frac{n}{P},(t+1)\cdot \frac{n}{P})$, with the final thread
extending its chunk all the way to $n$, to deal with the case where
$n$ is not divisible by $p$. For this problem there is no advantage to
being explicit, but it demonstrates the idea of explicit partitioning
in a simple setting.

All variables declared inside a parallel region are private to each
thread. For example, each thread has its own distinct \texttt{t}
variable. However, all variables declared outside the parallel region
will be visible to all threads, including any changes made to them.
Any memory allocated inside or outside the parallel region is also
accessible to all threads. We can use this to construct arbitrarily
complicated per-thread partial results.

\subsection{Summation with Parallel Regions}
\label{sec:parallel-summation}

We will now look at problems that are slightly less parallel, where
all elements of the input potentially contribute to each element of
the result.

The basic technique behind explicit work partioning is to allocate a
\emph{results array} with one element per (potential) thread, enter a
parallel region where each thread processes a chunk of the input and
writes a result to its corresponding element to the results array,
then after the final region we have a sequential loop that aggregates
the results array to a single final result.

\Cref{lst:openmp-partition-sum} shows an implementation of vector
summation using this technique. The integer array \texttt{sums}
contains an element for each potential thread. We use
\texttt{omp\_get\_max\_threads()} to retrieve the maximum number of
threads (\texttt{P}) that might potentially be used for the parallel
region. We must be careful \emph{not} to use
\texttt{omp\_get\_num\_threads()}, as this function returns the number
of threads used for the current parallel region - and as we are still
outside the parallel region when constructing the array,
\texttt{omp\_get\_num\_threads()} would return 1. The logic for
distributing the elements of the vector \texttt{A} is the same as we
saw in \cref{lst:openmp-vecadd-explicit}.

\begin{figure}
\lstinputlisting[
caption={Vector summation using explicit work distribution.},
label={lst:openmp-partition-sum},
language=C,
frame=single,
firstline=1,
lastline=37]
{src/openmp-partition-sum.c}
\end{figure}

\subsection{Filtering with parallel regions}

We have still accomplished nothing that could not also done with the
reductions explained in \cref{sec:reductions}. Let us now turn to a
problem that cannot be solved with reductions - specifically, let us
see how to filter an array to remove all negative elements, which as
we saw in \cref{sec:list-homomorphisms} is a list homomorphism.

First let us look at a sequential function for filtering lists. We
assume we are given an input array \texttt{src} of size \texttt{n}, as
well as an output array \texttt{dst} of at least size \texttt{n}. We
will copy the non-negative elements of \texttt{src} to \texttt{dst},
tallying how many have been copied in the variable \texttt{p}, and
finally return the number of elements written. The array \texttt{src}
can then be seen as a length-\texttt{p} array (with some trailing
garbage).

\lstinputlisting[
caption={Sequential filtering.},
label={lst:filter-sequential},
language=C,
frame=single,
firstline=6,
lastline=16]
{src/openmp-filter.c}

The idea behind a parallel filter is then to break the overall input
array apart into chunks, filter each chunk separately, then
concatenate the filtered chunks. We will represent the per-thread
results as \emph{two} arrays: an array of pointers to filtered arrays,
and an array of sizes of those arrays:

\begin{lstlisting}[
caption={constructing results arrays for parallel filtering.},
frame=single
]
int filter_openmp_limited(int n, int *dst, int *src) {
  int P = omp_get_max_threads();
  int* parts[P];
  int parts_n[P];

  for (int i = 0; i < P; i++) {
    parts[i] = NULL;
    parts_size[i] = 0;
  }

  // parallel region here...

  // combine parallel results here...
}
\end{lstlisting}

Now let us turn to the main parallel region. Computing which part of
the input array should be filtered by each thread is done largely as
in previous examples, with a computation based on the thread number
and the total number of threads. To do the actual filtering, we reuse
the sequential \texttt{filter} function from
\cref{lst:filter-sequential}. It is quite common in this kind of
parallel programming to reuse sequential components within each
thread, in order to take advantage of already-optimised (and correct)
code. Each thread must allocate memory for its output array, and to be
able to handle the case where \emph{no} elements are removed, the size
of this array must be equal to the chunk size.

\begin{lstlisting}[
caption={The main parallel region for a parallel filter.},
label=lst:parallel-filter-region,
frame=single
]
#pragma omp parallel
  {
    int t = omp_get_thread_num();
    int start = t * n/P;
    int end = (t+1) * n/P;
    if (t == omp_get_num_threads()-1) { end = n; }
    int chunk_size = end - start;

    parts[t] = malloc(sizeof(int) * chunk_size);
    parts_n[t] = filter(chunk_size, parts[t], src+start);
  }
\end{lstlisting}

We now face the problem of concatenating the \texttt{parts} arrays.
One straightforward solution is to simple do so sequentially, as
follows, where we also take care to deallocate the partial results
when we are done with them.

\begin{lstlisting}[
caption={Concatenating the filtered arrays, while also tallying up the total size of the result},
frame=single
]
  int p = 0;
  for (int i = 0; i < P; i++) {
    memcpy(dst+p, parts[i], parts_n[i]*sizeof(int));
    p += parts_n[i];
    free(parts[i]);
  }
  return p;
\end{lstlisting}

Although it only runs for \texttt{P} iterations, the \texttt{memcpy}
operation is likely to be expensive. Because this loop both reads and
writes the output index \texttt{p}, it \emph{must} be sequential. For
some filtering problems this may be acceptable: if we expect that the
vast majority of elements have been removed, or if the filtering
predicate itself is very expensive, then it may be sufficient to
parallelise the filtering operation as we did in
\cref{lst:parallel-filter-region}. But for other workloads, the cost
of concatenating the partial results will be just as computationally
costly as the filtering, and so we would be well served to parallelise
it as well.

To do so, we break the loop into two. First we compute an integer
array \texttt{offsets}, where \texttt{offsets[t]} tells us where
thread \texttt{t} should write its partial result in \texttt{dst}.
This can be done with a sequential loop with \texttt{P} iterations,
but because we do very little work in each iteration of the loop, this
is unlikely to have much impact.

\begin{minipage}{\linewidth}
\begin{lstlisting}[
caption={Constructing the offset array.},
frame=single
]
  int offsets[P];
  offsets[0] = 0;
  for (int t = 1; t < P; t++) {
    offsets[t] = offsets[t-1] + parts_n[t-1];
  }
\end{lstlisting}
\end{minipage}

Note that \lstinline{offsets[P-1]+parts_n[P-1]} is now the size of the
complete filtered array.

Now that we know the offsets for where each partial result should be
written in the destination array, it is straightforward to write a
parallel loop that does so.

\begin{lstlisting}[
caption={Concatenating the filtered arrays with a parallel loop.},
frame=single
]
#pragma omp parallel for
  for (int t = 0; t < P; t++) {
    memcpy(dst+offsets[t], parts[t],
           parts_n[t]*sizeof(int));
    free(parts[t]);
  }
\end{lstlisting}

We now have a fully parallelised filter. Although the details differ,
the overall structure is the same as we saw for summation in
\cref{sec:parallel-summation}---split the input into parts, compute an
array of partial results, then combine the partial results. For
filtering, the combination of partial results is particularly
complicated, and most problems you will encounter will be easier. The
full function is shown in \cref{lst:filter-parallel}.

\begin{figure}
\lstinputlisting[
caption={Fully parallel filtering.},
label={lst:filter-parallel},
language=C,
frame=single,
firstline=50,
lastline=86]
{src/openmp-filter.c}
\end{figure}

\section{The Big Concurrency Problems}

Concurrency and Parallel programming is a complex task, and can often
be a study in what not to do. Luckily, OpenMP manages a lot of the
underlying system interactions to ensure correct and efficient
parallelism, but no system is perfect. If you ever need to debug whats
going on, or try to implement your own concurrent solution you will
need a firm understanding of the two core problems; race conditions
and deadlocks.

\subsection{Races}

\begin{definition}[Race Condition]
  Sometimes also referred to simply as a race. A race condition is any processing whose final result depends on the arbitrary ordering of prior operations.
\end{definition}

Races are a result of concurrent programs least desirable feature, non-determinism. This is due to the essentially random\footnote{Note that in practice the scheduler will not be literally scheduling at random, but that process scheduling is way out of scope for this course. As user of a computer, we also have little to no control over other users and processes that our program may be competing with, so even when we do understand the scheduler it is still treated as effectively random.} scheduling of any threads and processes, so we do not actually know in what order concurrent operations will be performed.

Within threading this is most often caused by global variables being shared among several threads. This is as each thread can read and write to and from the same location, in essentially any order. As an example, consider the code in listing \ref{lst:race-example}. This code will spread the execution of the counting loop across many threads, which will all read, increment and write to count in an arbitrary order. This will produce an output for count that could be any value between 2 up to 1000000. In practice you are \textit{very} unlikely to get a result as low as 2, but it is technically possible if you get the very unluckiest scheduling\footnote{If you're interested in how this is possible you can look through the trace on page 36 of this \href{https://link.springer.com/content/pdf/10.1007/s00165-017-0447-x.pdf}{pdf}}. Of course its just as possible that you get 1000000 but this non-deterministic result is everything we want to avoid in computing. 

\begin{figure}
  \lstinputlisting[
  caption={An example of a racing program.},
  label={lst:race-example},
  language=C,
  frame=single]
  {src/race.c}
\end{figure}
  
\subsection{Locks}

The code in \ref{lst:race-example} will produce a non-deterministic result, which we have already seen how to solve in OpenMP, by adding 'reduction(+:count)' after the 'parallel for' loop. Reductions are great for OpenMP, but a more broadly applicable solution is the use of a mutex. Note that within the wider literature you will see references to mutexes, locks, or semaphores. These are all name for related but slightly different objects but within HPPS, and we will only concern ourselves with mutexes. They are effectively a flag that can only be set by one thread at a time. 

This is achieved via atomic operations. An atomic operation is one that cannot be interrupted. Recall that the problem with race conditions is that we cannot guarantee that a thread will not interrupt another thread, even if it is midway through an operation. Mutexes are implemented at the machine level to only have two operations, set (e.g. claim the mutex) and release (e.g. give up the mutex). Both of these operations are implemented as atomic operations, that the scheduler cannot interrupt. Once a thread has claimed a mutex, then any other threads that try to claim it will be blocked until the mutex is released. This can have the effect of reducing how much parallel processing is taking place, as threads will have to wait for mutexes to be released before they can continue. However, this cost to speed is worth it if it means we can get an actually meaningful result.

\begin{figure}
  \lstinputlisting[
  caption={An example of a mutex fixing a racing program.},
  label={lst:mutex-example},
  language=C,
  frame=single]
  {src/mutex.c}
\end{figure}

An example of how a mutex can be implemented using the 'pthread' library, as is shown in listing \ref{lst:mutex-example}. This example will always produce the correct result, regardless of how many threads are used to calculate it. Note that in this case, the mutex is locking access to the entirety of the parallelised section, meaning that this program will in fact be much slower than the racing program shown in listing \ref{lst:race-example}. We can see this in the timings presented below, when the mutexed version is 3 times slower, as the threads need to keep waiting for each other to release the mutex. This shows us that mutex usage should be minimised, by only locking the smallest number of instructions to ensure we get the correct result. Sometimes a lock is inevitable though, and so if we were going to improve this program we would perhaps restructure the entire code so that each thread calculates a subtotal, and these are then totalled in a mutexed variable. This is in fact what an OpenMP reduction automatically does.

\begin{minipage}{1.0\linewidth}
  \begin{verbatim}
    $ time ./race
    real    0m0.031s
    user    0m0.173s
    sys     0m0.004s
    $ time ./mutex
    real    0m0.102s
    user    0m0.161s
    sys     0m0.484s
  \end{verbatim}
\end{minipage}

\subsection{Deadlock}

Mutexes may help solve the problem of races, but they can introduce a compeletly new problem, deadlock. 

\begin{definition}[Deadlock]
  Any situation where no system progress can take place, as every process/thread is waiting for another to progress before it can.
\end{definition}

Much like races, deadlocks can be non deterministic which can make them twice as annoying to debug, but we must treat the possibility of a deadlock as though it will deadlock eventually. Therefore we need to design our systems so that they are deadlock free. As an example of how a deadlock could occur consider the following two threads, each which just lock and then unlock two mutexes each:


\begin{minipage}{1.0\linewidth}
  \begin{verbatim}
     Thread 1        Thread 2 
      lock(A)         lock(B)
      lock(B)         lock(A)
     unlock(A)       unlock(A)
     unlock(B)       unlock(B)
  \end{verbatim}
\end{minipage}

The ordering of these operations is completely up to the whims of the scheduler. Many orderings of these operations would be fine, but one such bad one would be if thread 1 locked mutex A, but was then immediately interrupted by thread 2 who would then lock mutex B. Thread 2 cannot continue as it cannot lock mutex A, as it is already locked by thread 1. Thread 1 also cannot continue as its next operations is to lock mutex B, but it is locked by thread 2. This is a deadlock, as there is no way for either thread to progress. There is also no way for these two threads to detect that they are in a deadlock, the system is completely stopped with no way to recover.

\subsection{Progress Graphs}

Reasoning about how each thread could be scheduled is all well and good, but a more robust method to identify potential deadlocks would help a lot. We can do so through the use of progress graphs. These are informal sketches, where we can map all the mutex interactions, and can then deduce any potentially deadlocking behaviour. For an example looking to figure \ref{fig:progress-deadlock}. In this type of graph we map each threads progression along the graph axis'. We only need to map any locking and unlocking operations, as anything else may be time consuming but is ultimately a non-blocking operation that will complete in a finite amount of time.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{img/progress-deadlock.pdf}
  \caption{A progress graph of the two potentially deadlocking threads.}
  \label{fig:progress-deadlock}
\end{figure}

As each thread is sequential, its progress can be mapped by following along its axis, with any point in the graph being an expression of the combined states of both graphs. For example, the bottom left corner would be neither thread having started yet, with the top right corner being both threads having completed. The shaded blue section shows forbidden zones, where due to our mutexes is an impossible state to reach. There are two overlapping zones here, one for mutex A and one for mutex B, with them each overlapping in the middle of the graph. We can derive the locations of these zones by taking the coordinate of where each thread locks the mutex as the bottom left corner, and the coordinate of where each thread unlocks the mutex as the top right corner.

Two traces have been shown through the graph, one in green and one in red. The green shows a valid route through that does not enter any forbidden zone and so will produce a valid result. The red is an impossible trace as it enters a state within the forbidden zones, and so cannot occur in practice.

The concerning part of this graph is the red shaded area towards the bottom left of the graph. This shows a potential deadlock. As each thread can only progress linearly, our route through the graph can only be parallel to either axis. If our state ever enters the red zone then there is no way to escape as progress is blocked by the two forbidden zones. This makes it trivial to say that if we can draw a progress graph, if it none of these progress traps exist then our system is deadlock free. A solution to this is to reorder the operations our threads perform. This can be seen in figure \ref{fig:progress-safe} where both threads are now locking and unlocking the mutexes in the same order. No progress traps exist and any valid state has at least one path out of, therefore we are always deadlock free.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{img/progress-safe.pdf}
  \caption{A progress graph of the two never deadlocking threads.}
  \label{fig:progress-safe}
\end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hpps-notes"
%%% End:
